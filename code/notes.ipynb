{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Main ICM Final Project Jupyter Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Justification for Choice of Images**\n",
    "\n",
    "We chose three qualitatively different images:\n",
    "\n",
    "- `antimeme.png`: A vector-like, clean image with large flat color regions and strong contrast. This makes it easier to visually assess blur and regularization artifacts, although the small “THIS IS FINE” text is very sensitive to blur.\n",
    "\n",
    "- `save_me.png`: A real-life photo with natural lighting, surface texture, sensor noise, and overlaid Chinese text plus artificial purple scribble. These factors introduce mixed frequencies and uneven illumination.\n",
    "\n",
    "- `this_is_fine.png`: A comic drawing that is not vectorized and contains bold ink lines, but also visible grain and scan noise. It lies between the other two in complexity: more texture than `antimeme.png` but cleaner than the real photo `save_me.png`.\n",
    "\n",
    "**At first**, we hypothesized that deconvolution difficulty would follow `save_me.png` > `this_is_fine.png` > `antimeme.png`, because natural photos usually contain more complex textures and noise.  \n",
    "\n",
    "However, the experiments in the Results section will show even though the overall error may be smaller, the tiny high-frequency text \"THIS IS FINE\" and smaller details of the main subject in `antimeme.png` actually are the hardest to recover under strong blur. This shows, actually, shadows and variation in images aren't the hardest part, but rather the size of the subjects at hand --- even vector-like components that are small are very hard to recover. Moreover, this shows how quantitative results such as MSE and PSNR may not accurately depict the perceptive effectiveness or meaningful image recovery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Justification for Choice of Private Key Image**\n",
    "\n",
    "`private_key.png` is a real-life photo, which means the natural lighting may create enough noise. As the photo subject is mainly green, this will ensure the kernels for R, G, and B will be different. All in all, these factors should make it a less predictable kernel. However, it shouldn't be too unpredictable to the point where recovery is hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>**Experiment Plan**</u>\n",
    "\n",
    "## **Experiments**\n",
    "\n",
    "Repeat the following kernels for all three images. We consider R, G, B as three separate channels needed for kernels and do not do grayscale, for better visual differentiation.\n",
    "\n",
    "### **Baseline: \"Transformations\" (No Private Key Convolutions)**\n",
    " - Gaussian Blur\n",
    " \n",
    " - Motion Blur\n",
    "\n",
    " - Box Blur\n",
    "\n",
    "### **New: \"Encryptions\" (Private Key Convolutions)**\n",
    " - Random Kernel from a seed (seed = private key), of course fix the seed across all three images\n",
    " \n",
    " - Private Key Image-derived Kernel via `private_key.png`\n",
    "\n",
    "## **Data Extracted**\n",
    "\n",
    "For each image + kernel + parameter setting, store:\n",
    "\n",
    "### Quantitative Data\n",
    " - **Image ID**: `antimeme`, `this_is_fine`, `save_me`\n",
    " \n",
    " - **Kernel type**: `Gaussian`, `Motion`, `Box`, `RandomSeed`, `KeyImage`\n",
    " \n",
    " - **Kernel parameters**: size, $\\sigma$ (for Gaussian), length/angle (for motion), seed, which key image\n",
    " - **Noise level** $\\sigma$ (for added Gaussian noise)\n",
    " \n",
    " - **Regularization parameter** $\\lambda$\n",
    "\n",
    " - **MSE** between original and recovered (over all RGB channels)\n",
    " \n",
    " - **PSNR** derived from MSE\n",
    "\n",
    "### Qualitative Data\n",
    " - **The Images**: original vs encrypted vs noisy vs recovered\n",
    " \n",
    " - Notes on **visual artifacts** such as ringing around edges, halo artifacts, oversmoothing, color shifts, \"ghosting\" shapes, complete failure (looks random)\n",
    "\n",
    "## **Methodology**\n",
    "\n",
    "### Summary\n",
    "This project studies how image reconstruction behaves under different convolution-based transformations, including both standard (public) kernels and “encrypted” private-key kernels. We evaluate reconstruction accuracy when the correct kernel is known, and investigate the sensitivity of the inverse problem by testing recovery with intentionally incorrect kernels. We do not perform blind kernel estimation; all recovery assumes access to either the true kernel or a deliberately mismatched one.\n",
    "\n",
    "### 1. Preprocessing\n",
    "- Load each image (`antimeme`, `this_is_fine`, `save_me`) as an RGB array of size $512 \\times 512 \\times 3$.\n",
    "- Normalize pixel values to $[0,1]$.\n",
    "- No grayscale conversion is performed; convolution is applied independently to each R, G, B channel.\n",
    "\n",
    "### 2. Apply Operator (Forward Model)\n",
    "For each image and each kernel type:\n",
    "\n",
    "#### Baseline Transformations (public kernels)\n",
    "These kernels are fully known:\n",
    "- Gaussian blur (fixed size and $\\sigma$)\n",
    "- Motion blur (fixed length and angle)\n",
    "- Box blur\n",
    "\n",
    "#### Encryption Operators (private-key kernels)\n",
    "These kernels are not publicly known:\n",
    "- Random kernel generated from a fixed seed (seed = private key)\n",
    "- Key-image-derived kernel from `private_key.png`\n",
    "\n",
    "Each kernel is applied channel-wise to form:\n",
    "$$\n",
    "y = Kx\n",
    "$$\n",
    "\n",
    "Then additive Gaussian noise is applied:\n",
    "$$\n",
    "z = y + \\text{noise}\n",
    "$$\n",
    "\n",
    "Noise is added to make the inverse problem realistic.\n",
    "\n",
    "### 3. Recovery (Inverse Problem)\n",
    "\n",
    "#### Correct-Kernel Recovery\n",
    "For each baseline or private-key kernel, perform non-blind deconvolution by solving:\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\arg\\min_x \\|Kx - z\\|^2 + \\lambda \\|x\\|^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $K$ = the known convolution operator\n",
    "- $\\lambda$ = regularization strength\n",
    "\n",
    "The solution is computed channel-wise using:\n",
    "- an FFT-based approximate inverse.\n",
    "\n",
    "#### Incorrect-Kernel Recovery (for encryption tests)\n",
    "To demonstrate key-dependence, we intentionally recover using a wrong kernel:\n",
    "- wrong random seed\n",
    "- wrong kernel size\n",
    "- Gaussian or motion blur instead of the private kernel\n",
    "\n",
    "We solve the same inverse problem using $\\tilde{K}$:\n",
    "\n",
    "$$\n",
    "\\hat{x}_{wrong} = \\arg\\min_x \\|\\tilde{K}x - z\\|^2 + \\lambda \\|x\\|^2\n",
    "$$\n",
    "\n",
    "This typically produces severe reconstruction failure and demonstrates that successful recovery requires the correct key.\n",
    "\n",
    "We do not attempt to guess or estimate kernels.\n",
    "\n",
    "### 4. Evaluation\n",
    "\n",
    "#### Quantitative Metrics\n",
    "- MSE between original and recovered RGB image:\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\hat{x}_i)^2\n",
    "$$\n",
    "\n",
    "- PSNR derived from MSE:\n",
    "$$\n",
    "\\text{PSNR} = 10 \\log_{10} \\left( \\frac{1}{\\text{MSE}} \\right)\n",
    "$$\n",
    "\n",
    "(assuming images are normalized to $[0,1]$).\n",
    "\n",
    "#### Recorded Metadata\n",
    "For each experiment, record:\n",
    "- kernel type\n",
    "- kernel parameters\n",
    "- noise level $\\sigma$\n",
    "- regularization $\\lambda$\n",
    "- whether the kernel used for recovery was correct or wrong\n",
    "\n",
    "#### Qualitative Comparison\n",
    "For each experiment, visually compare:\n",
    "- original image\n",
    "- transformed/encrypted image\n",
    "- noisy observation\n",
    "- recovered image (correct key)\n",
    "- recovered image (wrong key)\n",
    "\n",
    "and note artifacts such as ringing, oversmoothing, color distortion, or complete failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mathematical Formulation of the Code Implementation**\n",
    "\n",
    "### 0. Notation\n",
    "\n",
    "We model a color image as a discrete RGB function\n",
    "$$\n",
    "x(i,j,c), \\quad i,j \\in \\{0,\\dots,H-1\\}, \\; c \\in \\{R,G,B\\}.\n",
    "$$\n",
    "\n",
    "In this project we fix\n",
    "$$\n",
    "H = W = 512,\n",
    "$$\n",
    "so each image is of size $512 \\times 512 \\times 3$, and all pixel values are normalized to the range $[0,1]$.\n",
    "\n",
    "A (possibly color-dependent) convolution kernel is denoted by\n",
    "$$\n",
    "k_c(u,v), \\quad u,v \\in \\{-r,\\dots,r\\},\n",
    "$$\n",
    "where $c$ is the color channel and the kernel size is $(2r+1) \\times (2r+1)$.\n",
    "\n",
    "The 2D discrete convolution of an image channel $x_c$ with kernel $k_c$ is\n",
    "$$\n",
    "(K_c x_c)(i,j)\n",
    "= \\sum_{u=-r}^{r} \\sum_{v=-r}^{r} k_c(u,v)\\, x_c(i-u,\\, j-v),\n",
    "$$\n",
    "with indices handled by padding or circular wrapping in the code.\n",
    "\n",
    "For a kernel that is the same across channels (Gaussian, box, motion, random), we write $k(u,v)$ and\n",
    "$$\n",
    "(Kx)(i,j,c) = \\sum_{u,v} k(u,v)\\, x(i-u, j-v, c).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Convolution Kernels (corresponding to `kernels.py`)\n",
    "\n",
    "#### 1.1 Box Blur Kernel\n",
    "\n",
    "For a box blur of size \\(s \\times s\\), the kernel is constant:\n",
    "$$\n",
    "k_{\\text{box}}(u,v) = \\frac{1}{s^2}, \\quad u,v \\in \\left\\{ -\\frac{s-1}{2}, \\dots, \\frac{s-1}{2} \\right\\}.\n",
    "$$\n",
    "\n",
    "This corresponds to uniform averaging over an \\(s \\times s\\) neighborhood.\n",
    "\n",
    "#### 1.2 Gaussian Blur Kernel\n",
    "\n",
    "For a Gaussian blur of size $s \\times s$ and standard deviation $\\sigma$, we define\n",
    "$$\n",
    "g(u,v) = \\exp\\!\\left( -\\frac{u^2 + v^2}{2\\sigma^2} \\right),\n",
    "$$\n",
    "for $u,v$ in the appropriate index range, and then normalize:\n",
    "$$\n",
    "k_{\\text{gauss}}(u,v) = \\frac{g(u,v)}{\\sum_{u',v'} g(u',v')}.\n",
    "$$\n",
    "\n",
    "This ensures\n",
    "$$\n",
    "\\sum_{u,v} k_{\\text{gauss}}(u,v) = 1,\n",
    "$$\n",
    "so constant images remain constant under the blur.\n",
    "\n",
    "#### 1.3 Motion Blur Kernel\n",
    "\n",
    "A motion blur kernel of length $L$ and angle $\\theta$ is represented as a line of ones along a direction given by $\\theta$, normalized by its sum. Conceptually, in an $L \\times L$ grid:\n",
    "- initialize a line along the horizontal center row:\n",
    "  $$\n",
    "  k_0(u,v) =\n",
    "  \\begin{cases}\n",
    "  1, & \\text{if } u = 0 \\text{ and } v \\in \\{-\\tfrac{L-1}{2},\\dots,\\tfrac{L-1}{2}\\}, \\\\\n",
    "  0, & \\text{otherwise},\n",
    "  \\end{cases}\n",
    "  $$\n",
    "- rotate this pattern by angle $\\theta$,\n",
    "- then normalize:\n",
    "  $$ \n",
    "  k_{\\text{motion}}(u,v) = \\frac{k_{\\text{rot}}(u,v)}{\\sum_{u',v'} k_{\\text{rot}}(u',v')}.\n",
    "  $$\n",
    "\n",
    "This models a uniform motion along a line.\n",
    "\n",
    "#### 1.4 Random Kernel (Seeded)\n",
    "\n",
    "Let $s$ be the kernel size and $\\text{seed}$ a fixed integer. The kernel entries are generated as i.i.d. random values, then normalized:\n",
    "$$\n",
    "\\tilde{k}(u,v) \\sim \\text{Uniform}(0,1),\n",
    "$$\n",
    "$$\n",
    "k_{\\text{rand}}(u,v) = \\frac{\\tilde{k}(u,v)}{\\sum_{u',v'} \\tilde{k}(u',v')}.\n",
    "$$\n",
    "\n",
    "The fixed seed makes this kernel deterministic and repeatable, so it behaves as a \"private key\" known only to the experimenter.\n",
    "\n",
    "#### 1.5 RGB Key-Image-Derived Kernel\n",
    "\n",
    "Let the private key image be\n",
    "$$\n",
    "x_{\\text{key}}(i,j,c), \\quad c \\in \\{R,G,B\\}.\n",
    "$$\n",
    "\n",
    "For each channel $c$, we:\n",
    "1. Resize $x_{\\text{key}}(\\cdot,\\cdot,c)$ to a small kernel size $s \\times s$,\n",
    "2. Normalize it to sum to 1:\n",
    "   $$\n",
    "   k_c(u,v) = \\frac{x_{\\text{key, resized}}(u,v,c)}{\\sum_{u',v'} x_{\\text{key, resized}}(u',v',c)}.\n",
    "   $$\n",
    "\n",
    "This defines a **channel-dependent kernel**:\n",
    "$$\n",
    "K_c x_c = k_c * x_c,\n",
    "$$\n",
    "and the overall forward operator is\n",
    "$$\n",
    "(Kx)(i,j,c) = \\sum_{u,v} k_c(u,v)\\, x(i-u,j-v,c).\n",
    "$$\n",
    "\n",
    "This construction makes the encryption depend on the color structure of the private key image.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Forward Model (corresponding to `apply_kernel_fft`)\n",
    "\n",
    "For a given kernel (or set of kernels) $K$ and original image $x$, the noiseless forward model is\n",
    "$$\n",
    "y = Kx,\n",
    "$$\n",
    "meaning, per channel,\n",
    "$$\n",
    "y_c = K_c x_c = k_c * x_c.\n",
    "$$\n",
    "\n",
    "Additive Gaussian noise is then applied:\n",
    "$$\n",
    "z = y + \\eta,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\eta(i,j,c) \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2)\n",
    "$$\n",
    "independently for each pixel and channel. The observed image is thus\n",
    "$$\n",
    "z(i,j,c) = (K_c x_c)(i,j) + \\eta(i,j,c).\n",
    "$$\n",
    "\n",
    "In code, the forward convolution is implemented via FFT using the convolution theorem.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. FFT-Based Convolution and the Convolution Theorem (used in `fft_deconv.py`)\n",
    "\n",
    "For a 2D discrete signal $x(i,j)$ and kernel $k(u,v)$, define their 2D discrete Fourier transforms:\n",
    "$$\n",
    "\\hat{x}(\\omega_1,\\omega_2) = \\mathcal{F}\\{x\\}(\\omega_1,\\omega_2),\n",
    "$$\n",
    "$$\n",
    "\\hat{k}(\\omega_1,\\omega_2) = \\mathcal{F}\\{k\\}(\\omega_1,\\omega_2).\n",
    "$$\n",
    "\n",
    "The convolution theorem states:\n",
    "$$\n",
    "\\mathcal{F}\\{k * x\\}(\\omega_1,\\omega_2) = \\hat{k}(\\omega_1,\\omega_2)\\, \\hat{x}(\\omega_1,\\omega_2).\n",
    "$$\n",
    "\n",
    "In the code, this is implemented by:\n",
    "1. Padding the kernel \\(k\\) to the same spatial size as the image,\n",
    "2. Taking FFTs:\n",
    "   $$\n",
    "   \\hat{K} = \\mathcal{F}\\{k_{\\text{padded}}\\}, \\quad\n",
    "   \\hat{X}_c = \\mathcal{F}\\{x_c\\},\n",
    "   $$\n",
    "3. Multiplying in the frequency domain:\n",
    "   $$\n",
    "   \\hat{Y}_c = \\hat{K} \\cdot \\hat{X}_c,\n",
    "   $$\n",
    "4. Inverse FFT:\n",
    "   $$\n",
    "   y_c = \\mathcal{F}^{-1}\\{\\hat{Y}_c\\}.\n",
    "   $$\n",
    "\n",
    "This is what `apply_kernel_fft` does for each channel.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Regularized Inverse (Deconvolution) — `deconv_fft`\n",
    "\n",
    "#### 4.1 Tikhonov-Regularized Minimization Problem\n",
    "\n",
    "Given the observed image $z$ and known operator $K$, we solve a Tikhonov-regularized least squares problem:\n",
    "$$\n",
    "\\hat{x} = \\arg\\min_x \\|Kx - z\\|^2 + \\lambda \\|x\\|^2,\n",
    "$$\n",
    "where $\\lambda > 0$ is the regularization strength. For simplicity, we take $L = I$, so the penalty term is $\\|x\\|^2$.\n",
    "\n",
    "The normal equations for this problem are:\n",
    "$$\n",
    "(K^T K + \\lambda I)\\hat{x} = K^T z.\n",
    "$$\n",
    "\n",
    "Because $K$ is a convolution operator (assumed circulant after padding), it is diagonalized by the FFT.\n",
    "\n",
    "#### 4.2 Frequency-Domain Derivation\n",
    "\n",
    "Let $\\hat{K}(\\omega_1,\\omega_2)$ and $\\hat{Z}(\\omega_1,\\omega_2)$ be the FFTs of the kernel and the observed image channel. In the frequency domain, the normal equations become, for each frequency $(\\omega_1, \\omega_2)$:\n",
    "$$\n",
    "(|\\hat{K}(\\omega)|^2 + \\lambda)\\, \\hat{X}(\\omega)\n",
    "= \\overline{\\hat{K}(\\omega)}\\, \\hat{Z}(\\omega),\n",
    "$$\n",
    "so\n",
    "$$\n",
    "\\hat{X}(\\omega)\n",
    "= \\frac{\\overline{\\hat{K}(\\omega)}\\, \\hat{Z}(\\omega)}{|\\hat{K}(\\omega)|^2 + \\lambda}.\n",
    "$$\n",
    "\n",
    "This is the standard Tikhonov-regularized deconvolution (Wiener-like) formula.\n",
    "\n",
    "The recovered image channel is obtained by inverse FFT:\n",
    "$$\n",
    "\\hat{x}(i,j) = \\mathcal{F}^{-1}\\{\\hat{X}(\\omega)\\}(i,j).\n",
    "$$\n",
    "\n",
    "In the RGB case, this is applied independently to each channel:\n",
    "$$\n",
    "\\hat{X}_c(\\omega)\n",
    "= \\frac{\\overline{\\hat{K}_c(\\omega)}\\, \\hat{Z}_c(\\omega)}{|\\hat{K}_c(\\omega)|^2 + \\lambda},\n",
    "$$\n",
    "$$\n",
    "\\hat{x}_c = \\mathcal{F}^{-1}\\{\\hat{X}_c\\}.\n",
    "$$\n",
    "\n",
    "This is exactly what `deconv_fft` (and its RGB wrapper) implements.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Wrong-Kernel Recovery — Key Dependence\n",
    "\n",
    "For encryption experiments, the true forward operator is $K$, but we deliberately use an *incorrect* kernel $\\tilde{K}$ in the inverse:\n",
    "$$\n",
    "\\hat{x}_{\\text{wrong}} = \\arg\\min_x \\|\\tilde{K}x - z\\|^2 + \\lambda \\|x\\|^2,\n",
    "$$\n",
    "leading in the frequency domain to\n",
    "$$\n",
    "\\hat{X}_{\\text{wrong}}(\\omega)\n",
    "= \\frac{\\overline{\\hat{\\tilde{K}}(\\omega)}\\, \\hat{Z}(\\omega)}{|\\hat{\\tilde{K}}(\\omega)|^2 + \\lambda}.\n",
    "$$\n",
    "\n",
    "Since $\\tilde{K} \\neq K$, this corresponds to solving the wrong inverse problem and generally yields severe reconstruction artifacts or failure. This behavior demonstrates the dependence of successful recovery on the correct \"key\" (kernel).\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Error Metrics — `metrics.py`\n",
    "\n",
    "#### 6.1 Mean Squared Error (MSE)\n",
    "\n",
    "For two RGB images $x$ and $\\hat{x}$ of size $H \\times W \\times 3$, we flatten all channels and pixels into one vector of length $N = H \\cdot W \\cdot 3$, and define\n",
    "$$\n",
    "\\text{MSE}(x,\\hat{x}) = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\hat{x}_i)^2.\n",
    "$$\n",
    "\n",
    "This is implemented as `np.mean((a - b)**2)`.\n",
    "\n",
    "#### 6.2 Peak Signal-to-Noise Ratio (PSNR)\n",
    "\n",
    "Assuming images are normalized to the range $[0,1]$, the peak possible value is\n",
    "$$\n",
    "\\text{MAX} = 1.\n",
    "$$\n",
    "\n",
    "Then PSNR is defined as\n",
    "$$\n",
    "\\text{PSNR} = 10 \\log_{10} \\left( \\frac{\\text{MAX}^2}{\\text{MSE}} \\right)\n",
    "= 10 \\log_{10} \\left( \\frac{1}{\\text{MSE}} \\right).\n",
    "$$\n",
    "\n",
    "This is implemented as `10 * np.log10(1.0 / mse_value)`.\n",
    "\n",
    "High PSNR corresponds to good reconstruction; very low PSNR corresponds to failure, especially in the wrong-key experiments.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
